{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate queries\n",
    "\n",
    "Notebook that generates queries. This hasn't been made into a CL-script yet, but that seems to be OK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Prelims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import joblib\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from inspect import signature\n",
    "from os.path import dirname\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Custom imports\n",
    "\n",
    "root_dir = dirname(dirname(os.getcwd()))\n",
    "src_dir = os.path.join(root_dir, 'src')\n",
    "sys.path.append(src_dir)\n",
    "\n",
    "from exp.query.generation import *\n",
    "from exp.query.encoding import *\n",
    "from exp.utils.extra import generate_keychain\n",
    "from exp.utils import filesystem as fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from exp.runner.RunExp import RunExp\n",
    "from exp.runner.RunMercs import RunMercs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exp = RunExp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "default_io_config_dirs = exp.default_io_config_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def default_io_config_file(dirs=None, dataset=None, name=None):\n",
    "    d={}\n",
    "    \n",
    "    d['train_data'] = fs.collect_fnames_from_folder(dirs['resc-data-ds'],\n",
    "                                                    criteria=['Train'],\n",
    "                                                    indexed=True)\n",
    "    \n",
    "    if name is not None and dataset is not None:\n",
    "        assert isinstance(name, str)\n",
    "        \n",
    "        qry_codes_fname = generate_keychain(['qry', 'codes', name], sep='_')\n",
    "        d['qry-codes'] = fs.gen_derived_fnames(dataset,\n",
    "                                               name=qry_codes_fname,\n",
    "                                               extension='',\n",
    "                                               dname=dirs['resc-query-ds-codes'],\n",
    "                                               indexed=False)[0]\n",
    "        \n",
    "        qry_config_fname = generate_keychain(['qry','config', name], sep='_')\n",
    "        d['qry-config'] = fs.gen_derived_fnames(dataset,\n",
    "                                                name=qry_config_fname,\n",
    "                                                extension='json',\n",
    "                                                dname=dirs['resc-query-ds-config'],\n",
    "                                                indexed=False)[0]\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def default_query_parameters():\n",
    "    d = {'qry_mode':      'basic',\n",
    "         'random_seed':    997}\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_nb_atts(io_config):\n",
    "    data_fname = io_config['file']['train_data'][0][1]\n",
    "    df = pd.read_csv(data_fname)\n",
    "    nb_atts = len(df.columns)            \n",
    "    return nb_atts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_outputs(qry_codes, qry_config, io_config):\n",
    "    \"\"\"\n",
    "    Save query-codes and query-config to files.\n",
    "    \"\"\"\n",
    "    \n",
    "    qry_codes_fname = io_config['file']['qry-codes']\n",
    "    qry_config_fname = io_config['file']['qry-config']\n",
    "    \n",
    "    fs.ensure_dir(os.path.dirname(qry_codes_fname), empty=False)\n",
    "    fs.ensure_dir(os.path.dirname(qry_config_fname), empty=False)\n",
    "    \n",
    "    # Save both files\n",
    "    np.save(qry_codes_fname, qry_codes)\n",
    "    \n",
    "    with open(qry_config_fname, 'w') as f:\n",
    "        json.dump(qry_config, f, indent=4)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_qry_codes(query_config):\n",
    "    \"\"\"\n",
    "    Generate query codes\n",
    "    \"\"\"\n",
    "    \n",
    "    _, _, q_desc, q_targ, q_miss = compile_queries(**query_config)\n",
    "    codes = queries_to_codes(q_desc, q_targ, q_miss)\n",
    "    \n",
    "    return codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_config(**kwargs):\n",
    "    \"\"\"\n",
    "    Generate query config.\n",
    "    \"\"\"\n",
    "    \n",
    "    root_dir = kwargs['root_dir']\n",
    "    dataset = kwargs['dataset']\n",
    "    qry_mode = kwargs['mode']\n",
    "    qry_name = kwargs.get('name', qry_mode)\n",
    "    \n",
    "    config={}\n",
    "    config['io']={}\n",
    "    \n",
    "    config['io']['dirs'] = default_io_config_dirs(root_dir=root_dir,\n",
    "                                                  dataset=dataset)\n",
    "    config['io']['file'] = default_io_config_file(dirs=config['io']['dirs'],\n",
    "                                                  dataset=dataset,\n",
    "                                                  name=qry_name)\n",
    "    \n",
    "    nb_atts = extract_nb_atts(config['io'])\n",
    "    \n",
    "    # N.b., you start with defaults that may get overridden.\n",
    "    config['qry'] = {'nb_atts':  nb_atts,\n",
    "                     **default_query_parameters(),\n",
    "                     **kwargs}\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(config):\n",
    "    \n",
    "    # Extract config\n",
    "    io_config = config['io']\n",
    "    qry_config = config['qry']\n",
    "    \n",
    "    # Generate codes\n",
    "    qry_codes = generate_qry_codes(qry_config)\n",
    "    \n",
    "    save_outputs(qry_codes, qry_config, io_config)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factory-Floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = 'nltcs'\n",
    "query_mode = 'iterative'\n",
    "config = build_config(root_dir=root_dir, dataset=ds, mode=query_mode, name='it-S', nb_diff_configs=10)\n",
    "#config['io']['file']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry_config = config['qry']\n",
    "qry_codes = generate_qry_codes(qry_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0, -1,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0, -1, -1,  1,  0,  0,  0, -1,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0, -1, -1,  1,  0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0],\n",
       "       [ 0, -1, -1, -1,  1,  0,  0,  0, -1,  0,  0, -1,  0, -1,  0,  0],\n",
       "       [-1, -1, -1, -1,  1,  0,  0,  0, -1,  0,  0, -1,  0, -1,  0,  0],\n",
       "       [-1, -1, -1, -1,  1,  0,  0, -1, -1,  0,  0, -1,  0, -1, -1,  0],\n",
       "       [-1, -1, -1, -1,  1,  0,  0, -1, -1,  0,  0, -1,  0, -1, -1, -1],\n",
       "       [-1, -1, -1, -1,  1,  0,  0, -1, -1,  0, -1, -1, -1, -1, -1, -1],\n",
       "       [-1, -1, -1, -1,  1,  0,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qry_codes[0:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(qry_codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract datasets"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.where(res==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dirs = default_io_config_dirs(root_dir=root_dir)\n",
    "all_datasets = os.listdir(dirs['resc-data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adult',\n",
       " 'cwebkb',\n",
       " 'book',\n",
       " 'bbc',\n",
       " 'kdd',\n",
       " 'ad',\n",
       " 'msnbc',\n",
       " 'tretail',\n",
       " 'msweb',\n",
       " 'jester',\n",
       " 'pumsb_star',\n",
       " 'baudio',\n",
       " 'nltcs',\n",
       " 'plants',\n",
       " 'dna',\n",
       " 'bnetflix',\n",
       " 'voting',\n",
       " 'cr52',\n",
       " 'c20ng',\n",
       " 'kosarek',\n",
       " 'accidents',\n",
       " 'tmovie']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cmd': '/cw/dtailocal/Dropbox/Files/KUL/research/codebases/homework/cmd/',\n",
       " 'prod': '/cw/dtailocal/Dropbox/Files/KUL/research/codebases/homework/prod/',\n",
       " 'resc': '/cw/dtailocal/Dropbox/Files/KUL/research/codebases/homework/resc/',\n",
       " 'resc-data': '/cw/dtailocal/Dropbox/Files/KUL/research/codebases/homework/resc/data/tidy/',\n",
       " 'resc-models': '/cw/dtailocal/Dropbox/Files/KUL/research/codebases/homework/resc/models/',\n",
       " 'resc-query': '/cw/dtailocal/Dropbox/Files/KUL/research/codebases/homework/resc/query/',\n",
       " 'root': '/cw/dtailocal/Dropbox/Files/KUL/research/codebases/homework'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Parallel(n_jobs=8)(delayed(detect_constant_cols)(ds_dir) for ds in all_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adult',\n",
       " 'cwebkb',\n",
       " 'book',\n",
       " 'bbc',\n",
       " 'kdd',\n",
       " 'ad',\n",
       " 'msnbc',\n",
       " 'tretail',\n",
       " 'msweb',\n",
       " 'jester',\n",
       " 'pumsb_star',\n",
       " 'baudio',\n",
       " 'nltcs',\n",
       " 'plants',\n",
       " 'dna',\n",
       " 'bnetflix',\n",
       " 'voting',\n",
       " 'cr52',\n",
       " 'c20ng',\n",
       " 'kosarek',\n",
       " 'accidents',\n",
       " 'tmovie']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Done with query generation for dataset:    adult\n",
      "    Query mode was: iterative\n",
      "    \n",
      "\n",
      "    Done with query generation for dataset:    cwebkb\n",
      "    Query mode was: iterative\n",
      "    \n",
      "\n",
      "    Done with query generation for dataset:    book\n",
      "    Query mode was: iterative\n",
      "    \n",
      "\n",
      "    Done with query generation for dataset:    bbc\n",
      "    Query mode was: iterative\n",
      "    \n",
      "\n",
      "    Done with query generation for dataset:    kdd\n",
      "    Query mode was: iterative\n",
      "    \n",
      "\n",
      "    Done with query generation for dataset:    ad\n",
      "    Query mode was: iterative\n",
      "    \n",
      "\n",
      "    Done with query generation for dataset:    msnbc\n",
      "    Query mode was: iterative\n",
      "    \n",
      "\n",
      "    Done with query generation for dataset:    tretail\n",
      "    Query mode was: iterative\n",
      "    \n",
      "\n",
      "    Done with query generation for dataset:    msweb\n",
      "    Query mode was: iterative\n",
      "    \n",
      "\n",
      "    Done with query generation for dataset:    jester\n",
      "    Query mode was: iterative\n",
      "    \n",
      "\n",
      "    Done with query generation for dataset:    pumsb_star\n",
      "    Query mode was: iterative\n",
      "    \n",
      "\n",
      "    Done with query generation for dataset:    baudio\n",
      "    Query mode was: iterative\n",
      "    \n",
      "\n",
      "    Done with query generation for dataset:    nltcs\n",
      "    Query mode was: iterative\n",
      "    \n",
      "\n",
      "    Done with query generation for dataset:    plants\n",
      "    Query mode was: iterative\n",
      "    \n",
      "\n",
      "    Done with query generation for dataset:    dna\n",
      "    Query mode was: iterative\n",
      "    \n",
      "\n",
      "    Done with query generation for dataset:    bnetflix\n",
      "    Query mode was: iterative\n",
      "    \n",
      "\n",
      "    Done with query generation for dataset:    voting\n",
      "    Query mode was: iterative\n",
      "    \n",
      "\n",
      "    Done with query generation for dataset:    cr52\n",
      "    Query mode was: iterative\n",
      "    \n",
      "\n",
      "    Done with query generation for dataset:    c20ng\n",
      "    Query mode was: iterative\n",
      "    \n",
      "\n",
      "    Done with query generation for dataset:    kosarek\n",
      "    Query mode was: iterative\n",
      "    \n",
      "\n",
      "    Done with query generation for dataset:    accidents\n",
      "    Query mode was: iterative\n",
      "    \n",
      "\n",
      "    Done with query generation for dataset:    tmovie\n",
      "    Query mode was: iterative\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "for ds in all_datasets:\n",
    "    query_name = 'it-S'\n",
    "    query_mode = 'iterative'\n",
    "    config = build_config(root_dir=root_dir, dataset=ds, mode=query_mode, name=query_name, nb_diff_configs=10)\n",
    "    main(config)\n",
    "    \n",
    "    msg = \"\"\"\n",
    "    Done with query generation for dataset:    {}\n",
    "    Query mode was: {}\n",
    "    \"\"\".format(ds, query_mode)\n",
    "    print(msg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
